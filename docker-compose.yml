version: '3.8'

services:
  # Ollama service for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: godot-rag-ollama
    ports:
      - '11434:11434'
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:11434/api/tags']
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Main application service
  godot-docs-rag:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: godot-docs-rag-app
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./.env:/app/.env:ro
      - ./data:/app/data
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - default
    stdin_open: true
    tty: true

  # Model initialization service (runs once to pull models)
  ollama-init:
    image: ollama/ollama:latest
    container_name: godot-rag-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    command: >
      sh -c "
        echo 'Pulling required models...' &&
        ollama pull llama3.2:3b &&
        ollama pull llama3.1:8b &&
        echo 'Models ready!'
      "
    restart: 'no'

volumes:
  ollama_data:

networks:
  default:
    driver: bridge
