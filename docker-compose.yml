services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - '11435:11434'
    volumes:
      - ollama-data:/root/.ollama
      - ./entrypoint.sh:/entrypoint.sh:ro
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:--1} # Default to auto GPU layers
      - OLLAMA_MAX_LOADED_MODELS=3 # Allow multiple models in memory
      - OLLAMA_NUM_PARALLEL=2 # Allow parallel requests
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    entrypoint: ['/bin/bash', '/entrypoint.sh']
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'timeout', '10', 'ollama', 'list']
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s

    # Uncomment the following lines if you have NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # RAG Pipeline Service - runs complete pipeline with evaluation
  rag-app:
    build: .
    container_name: godot_rag_app
    volumes:
      - .:/app
      - ./data:/app/data # Persistent data storage
    environment:
      - CONFIG_PATH=/app/config.yaml
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONPATH=/app
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
    depends_on:
      ollama:
        condition: service_healthy # Wait for Ollama to be ready
    restart: "no" # Don't restart automatically for one-time evaluation run
    command: >
      sh -c "
        echo '🚀 Starting Godot RAG Pipeline + Evaluation in Docker...' &&
        echo '⏳ Waiting for Ollama to be fully ready...' &&
        sleep 45 &&
        echo '📊 Running complete pipeline with evaluation...' &&
        python pipeline_and_evaluation.py
      "

volumes:
  ollama-data:
